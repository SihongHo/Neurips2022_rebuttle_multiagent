{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file /opt/anaconda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /opt/anaconda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file /opt/anaconda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file /opt/anaconda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file /opt/anaconda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file /opt/anaconda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have testing results in 3 folders: Baseline_Project_MADDPG, Baseline_Project_M3DDPG, Myalgorithm_Bechmark. We use project_name to denote them.\n",
    "\n",
    "### We have 8 senarios in total. We use s1, ..., s8 to denote them.\n",
    "\n",
    "### We have 3 test types: Optimal, No, Random\n",
    "\n",
    "### file_name can be different in different projects:  eva_md, eva_m3, eva_ma\n",
    "\n",
    "### The benchmark files can be found in path: project_name/experiments/benchmark_files/file_name.pkl\n",
    "\n",
    "### For example : file_name = eva_ma_s1_e01_Optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "agent_num_list = [2, 2, 3, 3, 3, 2, 4, 4]\n",
    "print(len(agent_num_list))\n",
    "print(sum(agent_num_list))\n",
    "# Baseline_Project_MADDPG, Baseline_Project_M3DDPG, Myalgorithm_Bechmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we need? \n",
    "\n",
    "### We need to compute the mean and variance of testing data.\n",
    "\n",
    "### The following cells tell us how to get the data, how the data looks like with different file names. We also give an example to compute the sum of 96024 steps rewards for each agent in its senario.\n",
    "\n",
    "## What a table we need?\n",
    "\n",
    "### We need a table to compare the mean and variance of all these testing data. We have 3 * 3 * 8 .pkl files in total. And 3 * 3 * 23 agents' mean reward and variance to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location\n",
    "project_dic = {\"Myalgorithm_Bechmark\":\"ma\",'Baseline_Project_MADDPG':\"md\", 'Baseline_Project_M3DDPG':\"m3\"}\n",
    "noise_list = {\"No\",'Optimal','Random'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnlyReward(data):\n",
    "    agent_num = len(data[0][0][0])\n",
    "    rewards = [[] for i in range(agent_num)]\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i][0])):\n",
    "            for k in range(agent_num):\n",
    "                rewards[k].append(data[i][0][j][k])\n",
    "    return rewards\n",
    "\n",
    "def Tuple(data):\n",
    "    agent_num = len(data[0][0][0])\n",
    "    rewards = [[] for i in range(agent_num)]\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i][0])):\n",
    "            for k in range(agent_num):\n",
    "                rewards[k].append(data[i][0][j][k][0])\n",
    "    return rewards\n",
    "\n",
    "precision = 1000.0\n",
    "\n",
    "def Round(data):\n",
    "    return int(data*precision+0.5)/precision\n",
    "\n",
    "def RoundList(data):\n",
    "    res = []\n",
    "    for i in data:\n",
    "        res.append(int(i*precision+0.5)/precision)\n",
    "    return res\n",
    "\n",
    "scenario_dic = {\"s1\": OnlyReward, 's2': OnlyReward, \"s3\":Tuple, 's4': Tuple, 's5': Tuple, 's6': OnlyReward, 's7': Tuple, 's8': Tuple}\n",
    "adv_dic = {\"s1\": 0, 's2': 0, \"s3\":0, 's4': 1, 's5': 1, 's6': 1, 's7': 3, 's8': 4}\n",
    "player_dic = {\"s1\": 2, 's2': 2, \"s3\":3, 's4': 3, 's5': 3, 's6': 2, 's7': 4, 's8': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show mean and variance of average reward and rewards for each agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 'e01'\n",
    "for noise in noise_list:\n",
    "    for scenario, f in scenario_dic.items():\n",
    "        for project, algo in project_dic.items():\n",
    "            print(\"Algo: \" + algo + \", scenario: \"+ scenario + \", noise: \" + noise)\n",
    "            file_name = 'eva_' + algo + '_' + scenario + '_' + exp + '_' + noise\n",
    "            file_dic = './' + project + '/experiments/benchmark_files/' + file_name + '.pkl'\n",
    "            print(file_dic)\n",
    "            data = pickle.load(open(file_dic, 'rb'))\n",
    "            rewards = np.array(f(data))\n",
    "            print(\"Mean: \", Round(np.mean(rewards)), \"Mean for each agent: \", RoundList(np.mean(rewards, axis=1)))\n",
    "            print(\"Var: \", Round(np.var(rewards)), \"Var for each agent: \", RoundList(np.var(rewards, axis=1)))\n",
    "            print(\"=================================\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show mean and variance of average reward of all, good agents and adversaries for e01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rewards_for_one(noise, scenario, f, project, algo, exp):\n",
    "    print(\"Algo: \" + algo + \", scenario: \"+ scenario + \", noise: \" + noise)\n",
    "    file_name = 'eva_' + algo + '_' + scenario + '_' + exp + '_' + noise\n",
    "    file_dic = './' + project + '/experiments/benchmark_files/' + file_name + '.pkl'\n",
    "    print(file_dic)\n",
    "    data = pickle.load(open(file_dic, 'rb'))\n",
    "    rewards = np.array(f(data))\n",
    "    adv_num = adv_dic[scenario]\n",
    "    player_num = player_dic[scenario]\n",
    "    print(\"Mean: \", Round(np.mean(rewards)))\n",
    "    print(\"Var: \", Round(np.var(rewards)))\n",
    "    if adv_num != 0:\n",
    "        print(\"Mean for adv: \", Round(np.mean(rewards[:adv_num, :])))\n",
    "        print(\"Var for adv: \", Round(np.var(rewards[:adv_num, :])))\n",
    "    print(\"Mean for agent: \", Round(np.mean(rewards[adv_num:player_num, :])))\n",
    "    print(\"Var for agent: \", Round(np.var(rewards[adv_num:player_num, :])))\n",
    "    print(\"=================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 'e01'\n",
    "for noise in noise_list:\n",
    "    for scenario, f in scenario_dic.items():\n",
    "        for project, algo in project_dic.items():\n",
    "            print_rewards_for_one(noise, scenario, f, project, algo, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show mean and variance average reward of all, good agents and adversaries for e012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: ma, scenario: s1, noise: No\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s1_e12_No.pkl\n",
      "Mean:  -0.643\n",
      "Var:  0.383\n",
      "Mean for agent:  -0.643\n",
      "Var for agent:  0.383\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s2, noise: No\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s2_e12_No.pkl\n",
      "Mean:  -0.579\n",
      "Var:  0.34\n",
      "Mean for agent:  -0.579\n",
      "Var for agent:  0.34\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s3, noise: No\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s3_e12_No.pkl\n",
      "Mean:  -1.975\n",
      "Var:  0.413\n",
      "Mean for agent:  -1.975\n",
      "Var for agent:  0.413\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s4, noise: No\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s4_e12_No.pkl\n",
      "Mean:  -0.026\n",
      "Var:  0.393\n",
      "Mean for adv:  -0.544\n",
      "Var for adv:  0.446\n",
      "Mean for agent:  0.232\n",
      "Var for agent:  0.165\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s5, noise: No\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s5_e12_No.pkl\n",
      "Mean:  -0.227\n",
      "Var:  0.682\n",
      "Mean for adv:  -0.679\n",
      "Var for adv:  0.375\n",
      "Mean for agent:  -0.001\n",
      "Var for agent:  0.682\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s6, noise: No\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s6_e12_No.pkl\n",
      "Mean:  -0.234\n",
      "Var:  0.142\n",
      "Mean for adv:  -0.112\n",
      "Var for adv:  0.121\n",
      "Mean for agent:  -0.357\n",
      "Var for agent:  0.134\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s7, noise: No\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s7_e12_No.pkl\n",
      "Mean:  0.164\n",
      "Var:  4.991\n",
      "Mean for adv:  0.448\n",
      "Var for adv:  4.619\n",
      "Mean for agent:  -0.686\n",
      "Var for agent:  5.139\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s8, noise: No\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s8_e12_No.pkl\n",
      "Mean:  0.292\n",
      "Var:  2.738\n",
      "Mean for adv:  0.571\n",
      "Var for adv:  3.087\n",
      "Mean for agent:  -0.266\n",
      "Var for agent:  1.57\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s1, noise: Random\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s1_e12_Random.pkl\n",
      "Mean:  -0.824\n",
      "Var:  0.592\n",
      "Mean for agent:  -0.824\n",
      "Var for agent:  0.592\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s2, noise: Random\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s2_e12_Random.pkl\n",
      "Mean:  -0.701\n",
      "Var:  0.477\n",
      "Mean for agent:  -0.701\n",
      "Var for agent:  0.477\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s3, noise: Random\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s3_e12_Random.pkl\n",
      "Mean:  -2.304\n",
      "Var:  0.336\n",
      "Mean for agent:  -2.304\n",
      "Var for agent:  0.336\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s4, noise: Random\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s4_e12_Random.pkl\n",
      "Mean:  -0.216\n",
      "Var:  0.486\n",
      "Mean for adv:  -0.68\n",
      "Var for adv:  0.551\n",
      "Mean for agent:  0.016\n",
      "Var for agent:  0.292\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s5, noise: Random\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s5_e12_Random.pkl\n",
      "Mean:  -0.269\n",
      "Var:  1.143\n",
      "Mean for adv:  -0.822\n",
      "Var for adv:  0.688\n",
      "Mean for agent:  0.006\n",
      "Var for agent:  1.142\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s6, noise: Random\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s6_e12_Random.pkl\n",
      "Mean:  -0.318\n",
      "Var:  0.194\n",
      "Mean for adv:  -0.152\n",
      "Var for adv:  0.178\n",
      "Mean for agent:  -0.484\n",
      "Var for agent:  0.155\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s7, noise: Random\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s7_e12_Random.pkl\n",
      "Mean:  0.211\n",
      "Var:  4.768\n",
      "Mean for adv:  0.466\n",
      "Var for adv:  4.553\n",
      "Mean for agent:  -0.553\n",
      "Var for agent:  4.629\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s8, noise: Random\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s8_e12_Random.pkl\n",
      "Mean:  0.092\n",
      "Var:  2.205\n",
      "Mean for adv:  0.373\n",
      "Var for adv:  2.165\n",
      "Mean for agent:  -0.47\n",
      "Var for agent:  1.81\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s1, noise: Optimal\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s1_e12_Optimal.pkl\n",
      "Mean:  -1.104\n",
      "Var:  1.007\n",
      "Mean for agent:  -1.104\n",
      "Var for agent:  1.007\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s2, noise: Optimal\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s2_e12_Optimal.pkl\n",
      "Mean:  -0.793\n",
      "Var:  0.56\n",
      "Mean for agent:  -0.793\n",
      "Var for agent:  0.56\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s3, noise: Optimal\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s3_e12_Optimal.pkl\n",
      "Mean:  -2.702\n",
      "Var:  0.322\n",
      "Mean for agent:  -2.702\n",
      "Var for agent:  0.322\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s4, noise: Optimal\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s4_e12_Optimal.pkl\n",
      "Mean:  -0.114\n",
      "Var:  0.583\n",
      "Mean for adv:  -0.798\n",
      "Var for adv:  0.595\n",
      "Mean for agent:  0.228\n",
      "Var for agent:  0.225\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s5, noise: Optimal\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s5_e12_Optimal.pkl\n",
      "Mean:  -0.253\n",
      "Var:  1.441\n",
      "Mean for adv:  -0.882\n",
      "Var for adv:  0.817\n",
      "Mean for agent:  0.06\n",
      "Var for agent:  1.456\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s6, noise: Optimal\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s6_e12_Optimal.pkl\n",
      "Mean:  -0.417\n",
      "Var:  0.323\n",
      "Mean for adv:  -0.083\n",
      "Var for adv:  0.263\n",
      "Mean for agent:  -0.751\n",
      "Var for agent:  0.161\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s7, noise: Optimal\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s7_e12_Optimal.pkl\n",
      "Mean:  0.137\n",
      "Var:  3.259\n",
      "Mean for adv:  0.321\n",
      "Var for adv:  3.139\n",
      "Mean for agent:  -0.414\n",
      "Var for agent:  3.213\n",
      "=================================\n",
      "\n",
      "Algo: ma, scenario: s8, noise: Optimal\n",
      "./Myalgorithm_Bechmark_D/experiments/benchmark_files/eva_ma_s8_e12_Optimal.pkl\n",
      "Mean:  -0.034\n",
      "Var:  1.543\n",
      "Mean for adv:  0.184\n",
      "Var for adv:  1.301\n",
      "Mean for agent:  -0.473\n",
      "Var for agent:  1.737\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = 'e12'\n",
    "for noise in noise_list:\n",
    "    for scenario, f in scenario_dic.items():\n",
    "        project, algo = \"Myalgorithm_Bechmark_D\", \"ma\"\n",
    "        print_rewards_for_one(noise, scenario, f, project, algo, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: m3, scenario: s1, noise: Optimal\n",
      "./Baseline_Project_M3DDPG/experiments/benchmark_files/eva_m3_s1_e12_Optimal.pkl\n",
      "Mean:  -1.245\n",
      "Var:  1.311\n",
      "Mean for agent:  -1.245\n",
      "Var for agent:  1.311\n",
      "=================================\n",
      "\n",
      "Algo: m3, scenario: s2, noise: Optimal\n",
      "./Baseline_Project_M3DDPG/experiments/benchmark_files/eva_m3_s2_e12_Optimal.pkl\n",
      "Mean:  -0.822\n",
      "Var:  0.592\n",
      "Mean for agent:  -0.822\n",
      "Var for agent:  0.592\n",
      "=================================\n",
      "\n",
      "Algo: m3, scenario: s3, noise: Optimal\n",
      "./Baseline_Project_M3DDPG/experiments/benchmark_files/eva_m3_s3_e12_Optimal.pkl\n",
      "Mean:  -2.817\n",
      "Var:  0.357\n",
      "Mean for agent:  -2.817\n",
      "Var for agent:  0.357\n",
      "=================================\n",
      "\n",
      "Algo: m3, scenario: s4, noise: Optimal\n",
      "./Baseline_Project_M3DDPG/experiments/benchmark_files/eva_m3_s4_e12_Optimal.pkl\n",
      "Mean:  -0.142\n",
      "Var:  0.644\n",
      "Mean for adv:  -0.886\n",
      "Var for adv:  0.665\n",
      "Mean for agent:  0.23\n",
      "Var for agent:  0.218\n",
      "=================================\n",
      "\n",
      "Algo: m3, scenario: s5, noise: Optimal\n",
      "./Baseline_Project_M3DDPG/experiments/benchmark_files/eva_m3_s5_e12_Optimal.pkl\n",
      "Mean:  -0.289\n",
      "Var:  1.721\n",
      "Mean for adv:  -0.944\n",
      "Var for adv:  0.955\n",
      "Mean for agent:  0.038\n",
      "Var for agent:  1.782\n",
      "=================================\n",
      "\n",
      "Algo: m3, scenario: s6, noise: Optimal\n",
      "./Baseline_Project_M3DDPG/experiments/benchmark_files/eva_m3_s6_e12_Optimal.pkl\n",
      "Mean:  -0.415\n",
      "Var:  0.401\n",
      "Mean for adv:  -0.014\n",
      "Var for adv:  0.311\n",
      "Mean for agent:  -0.817\n",
      "Var for agent:  0.168\n",
      "=================================\n",
      "\n",
      "Algo: m3, scenario: s7, noise: Optimal\n",
      "./Baseline_Project_M3DDPG/experiments/benchmark_files/eva_m3_s7_e12_Optimal.pkl\n",
      "Mean:  0.097\n",
      "Var:  2.816\n",
      "Mean for adv:  0.274\n",
      "Var for adv:  2.692\n",
      "Mean for agent:  -0.433\n",
      "Var for agent:  2.812\n",
      "=================================\n",
      "\n",
      "Algo: m3, scenario: s8, noise: Optimal\n",
      "./Baseline_Project_M3DDPG/experiments/benchmark_files/eva_m3_s8_e12_Optimal.pkl\n",
      "Mean:  -0.013\n",
      "Var:  1.706\n",
      "Mean for adv:  0.227\n",
      "Var for adv:  1.485\n",
      "Mean for agent:  -0.495\n",
      "Var for agent:  1.8\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = 'e12'\n",
    "for scenario, f in scenario_dic.items():\n",
    "    noise = \"Optimal\"\n",
    "    project, algo = \"Baseline_Project_M3DDPG\", \"m3\"\n",
    "    print_rewards_for_one(noise, scenario, f, project, algo, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: md, scenario: s1, noise: Optimal\n",
      "./Baseline_Project_MADDPG/experiments/benchmark_files/eva_md_s1_e12_Optimal.pkl\n",
      "Mean:  -1.266\n",
      "Var:  1.292\n",
      "Mean for agent:  -1.266\n",
      "Var for agent:  1.292\n",
      "=================================\n",
      "\n",
      "Algo: md, scenario: s2, noise: Optimal\n",
      "./Baseline_Project_MADDPG/experiments/benchmark_files/eva_md_s2_e12_Optimal.pkl\n",
      "Mean:  -0.878\n",
      "Var:  0.654\n",
      "Mean for agent:  -0.878\n",
      "Var for agent:  0.654\n",
      "=================================\n",
      "\n",
      "Algo: md, scenario: s3, noise: Optimal\n",
      "./Baseline_Project_MADDPG/experiments/benchmark_files/eva_md_s3_e12_Optimal.pkl\n",
      "Mean:  -2.818\n",
      "Var:  0.351\n",
      "Mean for agent:  -2.818\n",
      "Var for agent:  0.351\n",
      "=================================\n",
      "\n",
      "Algo: md, scenario: s4, noise: Optimal\n",
      "./Baseline_Project_MADDPG/experiments/benchmark_files/eva_md_s4_e12_Optimal.pkl\n",
      "Mean:  -0.141\n",
      "Var:  0.547\n",
      "Mean for adv:  -0.792\n",
      "Var for adv:  0.573\n",
      "Mean for agent:  0.183\n",
      "Var for agent:  0.217\n",
      "=================================\n",
      "\n",
      "Algo: md, scenario: s5, noise: Optimal\n",
      "./Baseline_Project_MADDPG/experiments/benchmark_files/eva_md_s5_e12_Optimal.pkl\n",
      "Mean:  -0.328\n",
      "Var:  1.256\n",
      "Mean for adv:  -0.892\n",
      "Var for adv:  0.859\n",
      "Mean for agent:  -0.046\n",
      "Var for agent:  1.216\n",
      "=================================\n",
      "\n",
      "Algo: md, scenario: s6, noise: Optimal\n",
      "./Baseline_Project_MADDPG/experiments/benchmark_files/eva_md_s6_e12_Optimal.pkl\n",
      "Mean:  -0.425\n",
      "Var:  0.445\n",
      "Mean for adv:  0.018\n",
      "Var for adv:  0.32\n",
      "Mean for agent:  -0.869\n",
      "Var for agent:  0.175\n",
      "=================================\n",
      "\n",
      "Algo: md, scenario: s7, noise: Optimal\n",
      "./Baseline_Project_MADDPG/experiments/benchmark_files/eva_md_s7_e12_Optimal.pkl\n",
      "Mean:  0.167\n",
      "Var:  3.748\n",
      "Mean for adv:  0.367\n",
      "Var for adv:  3.613\n",
      "Mean for agent:  -0.434\n",
      "Var for agent:  3.671\n",
      "=================================\n",
      "\n",
      "Algo: md, scenario: s8, noise: Optimal\n",
      "./Baseline_Project_MADDPG/experiments/benchmark_files/eva_md_s8_e12_Optimal.pkl\n",
      "Mean:  0.001\n",
      "Var:  1.547\n",
      "Mean for adv:  0.207\n",
      "Var for adv:  1.399\n",
      "Mean for agent:  -0.411\n",
      "Var for agent:  1.589\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = 'e12'\n",
    "for scenario, f in scenario_dic.items():\n",
    "    noise = \"Optimal\"\n",
    "    project, algo = \"Baseline_Project_MADDPG\", \"md\"\n",
    "    print_rewards_for_one(noise, scenario, f, project, algo, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4001 1 24 3\n"
     ]
    }
   ],
   "source": [
    "project_name = 'Myalgorithm_Bechmark'\n",
    "file_name = 'eva_ma_s3_e01_Optimal'\n",
    "# s1 s2 s6\n",
    "file_dic = './' + project_name + '/experiments/benchmark_files/' + file_name + '.pkl'\n",
    "data = pickle.load(open(file_dic, 'rb'))\n",
    "print(len(data), len(data[0]), len(data[0][0]), len(data[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4001, 1)\n",
      "2\n",
      "4001\n",
      "1\n",
      "24\n",
      "2\n",
      "[-0.9834718391222921, -0.3812060762704059]\n",
      "--------------------\n",
      "-110162.26950831478\n",
      "-86821.08641972108\n",
      "0\n",
      "0\n",
      "96024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "rewards = {}\n",
    "project_name = 'Myalgorithm_Bechmark'\n",
    "file_name = 'eva_ma_s1_e01_Optimal'\n",
    "# s1 s2 s6\n",
    "file_dic = './' + project_name + '/experiments/benchmark_files/' + file_name + '.pkl'\n",
    "if os.path.getsize(file_dic) > 0:      \n",
    "        with open(file_dic, \"rb\") as f:\n",
    "            unpickler = pickle.Unpickler(f)\n",
    "            # if file is not empty scores will be equal\n",
    "            # to the value unpickled\n",
    "            rewards = unpickler.load()\n",
    "print(np.array(rewards).shape)\n",
    "dim = len(rewards[0][0][0])\n",
    "print(dim)\n",
    "print(len(rewards))\n",
    "print(len(rewards[0]))\n",
    "print(len(rewards[0][0]))\n",
    "print(len(rewards[0][0][0]))\n",
    "print((rewards[0][0][0]))\n",
    "print(\"--------------------\")\n",
    "r1, r2, r3, r4 = 0, 0, 0, 0\n",
    "count = 0\n",
    "for i in range(0,4001):\n",
    "    for j in range(0,24):\n",
    "        r1 = r1 + rewards[i][0][j][0]\n",
    "        r2 = r2 + rewards[i][0][j][1]\n",
    "        count += 1\n",
    "print(r1)\n",
    "print(r2)\n",
    "print(r3)\n",
    "print(r4)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4001\n",
      "1\n",
      "24\n",
      "3\n",
      "[(-1.9999999942192552, array([1.0000000e+00, 2.8903724e-09, 3.4815282e-29, 1.7835960e-24],\n",
      "      dtype=float32), array([0., 1., 0., 0.])), (1.9999999942192552, array([0., 1., 0., 0.], dtype=float32), array([0., 1., 0., 0.])), (1.9999999942192552, array([1.4892894e-06, 9.9833727e-01, 1.2996176e-35, 1.6611381e-03],\n",
      "      dtype=float32), array([0., 1., 0., 0.]))]\n",
      "--------------------\n",
      "-93507.83140616429\n",
      "-1140.994853815925\n",
      "-1140.994853815925\n",
      "0\n",
      "96024\n"
     ]
    }
   ],
   "source": [
    "rewards = {}\n",
    "project_name = 'Baseline_Project_M3DDPG'\n",
    "file_name = 'eva_m3_s5_e01_Optimal'\n",
    "# s3 s4 s5\n",
    "file_dic = './' + project_name + '/experiments/benchmark_files/' + file_name + '.pkl'\n",
    "if os.path.getsize(file_dic) > 0:      \n",
    "        with open(file_dic, \"rb\") as f:\n",
    "            unpickler = pickle.Unpickler(f)\n",
    "            # if file is not empty scores will be equal\n",
    "            # to the value unpickled\n",
    "            rewards = unpickler.load()\n",
    "dim = len(rewards[0][0][0])\n",
    "print(dim)\n",
    "print(len(rewards))\n",
    "print(len(rewards[0]))\n",
    "print(len(rewards[0][0]))\n",
    "print(len(rewards[0][0][0]))\n",
    "print((rewards[0][0][0]))\n",
    "print(\"--------------------\")\n",
    "r1, r2, r3, r4 = 0, 0, 0, 0\n",
    "count = 0\n",
    "for i in range(0,4001):\n",
    "    for j in range(0,24):\n",
    "        r1 = r1 + rewards[i][0][j][0][0]\n",
    "        r2 = r2 + rewards[i][0][j][1][0]\n",
    "        r3 = r3 + rewards[i][0][j][2][0]\n",
    "        # r4 = r4 + rewards[i][0][j][3][0]\n",
    "        count += 1\n",
    "print(r1)\n",
    "print(r2)\n",
    "print(r3)\n",
    "print(r4)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4001\n",
      "1\n",
      "24\n",
      "4\n",
      "[(0, 0), (0, 0), (0, 0), (0, 0)]\n",
      "--------------------\n",
      "25860\n",
      "25860\n",
      "25860\n",
      "-33317.48512032645\n",
      "96024\n"
     ]
    }
   ],
   "source": [
    "rewards = {}\n",
    "project_name = 'Baseline_Project_M3DDPG'\n",
    "file_name = 'eva_m3_s7_e01_Optimal'\n",
    "# s7\n",
    "file_dic = './' + project_name + '/experiments/benchmark_files/' + file_name + '.pkl'\n",
    "if os.path.getsize(file_dic) > 0:      \n",
    "        with open(file_dic, \"rb\") as f:\n",
    "            unpickler = pickle.Unpickler(f)\n",
    "            # if file is not empty scores will be equal\n",
    "            # to the value unpickled\n",
    "            rewards = unpickler.load()\n",
    "dim = len(rewards[0][0][0])\n",
    "print(dim)\n",
    "print(len(rewards))\n",
    "print(len(rewards[0]))\n",
    "print(len(rewards[0][0]))\n",
    "print(len(rewards[0][0][0]))\n",
    "print((rewards[0][0][0]))\n",
    "print(\"--------------------\")\n",
    "r1, r2, r3, r4 = 0, 0, 0, 0\n",
    "count = 0\n",
    "for i in range(0,4001):\n",
    "    for j in range(0,24):\n",
    "        r1 = r1 + rewards[i][0][j][0][0]\n",
    "        r2 = r2 + rewards[i][0][j][1][0]\n",
    "        r3 = r3 + rewards[i][0][j][2][0]\n",
    "        r4 = r4 + rewards[i][0][j][3][0]\n",
    "        count += 1\n",
    "print(r1)\n",
    "print(r2)\n",
    "print(r3)\n",
    "print(r4)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4001\n",
      "1\n",
      "24\n",
      "6\n",
      "[(-0.07639192681351874, 0), (-0.038096391533437524, 0), (-0.11112985024669596, 0), (-0.042669555646066025, 0), (0.04093585563882962, 0), (0.030574316754278977, 0)]\n",
      "--------------------\n",
      "27185.89059806376\n",
      "27453.73009503019\n",
      "27746.891329523594\n",
      "27474.279627550433\n",
      "-51116.54909278312\n",
      "-25823.319954793806\n",
      "96024\n"
     ]
    }
   ],
   "source": [
    "rewards = {}\n",
    "project_name = 'Baseline_Project_M3DDPG'\n",
    "file_name = 'eva_m3_s8_e01_Optimal'\n",
    "# s8\n",
    "file_dic = './' + project_name + '/experiments/benchmark_files/' + file_name + '.pkl'\n",
    "if os.path.getsize(file_dic) > 0:      \n",
    "        with open(file_dic, \"rb\") as f:\n",
    "            unpickler = pickle.Unpickler(f)\n",
    "            # if file is not empty scores will be equal\n",
    "            # to the value unpickled\n",
    "            rewards = unpickler.load()\n",
    "dim = len(rewards[0][0][0])\n",
    "print(dim)\n",
    "print(len(rewards))\n",
    "print(len(rewards[0]))\n",
    "print(len(rewards[0][0]))\n",
    "print(len(rewards[0][0][0]))\n",
    "print((rewards[0][0][0]))\n",
    "print(\"--------------------\")\n",
    "r1, r2, r3, r4, r5, r6 = 0, 0, 0, 0, 0, 0\n",
    "count = 0\n",
    "for i in range(0,4001):\n",
    "    for j in range(0,24):\n",
    "        r1 = r1 + rewards[i][0][j][0][0]\n",
    "        r2 = r2 + rewards[i][0][j][1][0]\n",
    "        r3 = r3 + rewards[i][0][j][2][0]\n",
    "        r4 = r4 + rewards[i][0][j][3][0]\n",
    "        r5 = r5 + rewards[i][0][j][4][0]\n",
    "        r6 = r6 + rewards[i][0][j][5][0]\n",
    "        count += 1\n",
    "print(r1)\n",
    "print(r2)\n",
    "print(r3)\n",
    "print(r4)\n",
    "print(r5)\n",
    "print(r6)\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ea1916ba77a8ccce6e1d32e626899833938a2712c86908bdef9d7973b0a757c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
